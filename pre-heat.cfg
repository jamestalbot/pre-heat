[files]
# to scan: file to log urls that still need to be scanned for urls
to_scan: to_scan.urls
# scanned: file to log urls (and durations) that have been scanned
#   also used to prevent newly discovered urls from being added to the queue
scanned: scanned.csv
# errors: file to log urls and any error message encountered while scanning the url
errors:  errors.csv

[workers]
# workers: specify number of subprocesses to spin up doing network requests
pool_size: 5
# gather: specify whether or not to gather new URLs from pages as they are requested
gather: True

[criteria]
# start_url: specify the first url to hit each time a new connection is made
start_url: http://myhost.mydomain.tld/my/cool/url
# url roots: specify relatvie url inclusion criteria
#     pre-heat does not scan urls external to the original site/domain.
#     urls are only kept if they begin with one of the roots specified.
#     for example, if you have /foo and /bar as roots and you discover
#     links to urls /foob and /barf, both will be scanned.
#     but if you discover links to urls /afoo and /bbar, neither will
#     be scanned.
#     a url root of / will cause all urls linked from the site to the
#     site to be scanned
url_roots:
    /foo
    /bar

[display]
# dislpay each url as it is scanned for links? (default: True)
urls: False
# display scanned, to-scan, and error counts? (defaults: True)
scanned: True
to_scan: True
errors: True
# display counts after every ___ urls (default: 1)
every: 10
# display timing details? (default: False)
timing: True

